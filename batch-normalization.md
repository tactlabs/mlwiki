/ [Home](index.md)

# Batch Normalization

Batch Normalization is the process of normalizing the input or output of the activation functions in a hidden layer. Since a neural network is trained using a collected set of input called batch, the normalizing process also takes place in batches.

Advantages:

- Faster learning rate
- Reduces the dropout rate(data lost between layers), thus increasing accuracy significantly throughout the network.

<br>

**Created by Santhosh Kannan**

---

<br>
