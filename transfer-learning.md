/ [Home](index.md)

# transfer learning

●If you are using a specific NN architecture that has been trained before, you can use this pretrainedparameters/weights instead of random initialization to solve your problem.
●It can help you boost the performance of the NN.
●The pretrained models might have trained on a large datasets like ImageNet, Ms COCO, or pascaland took a lot of time to learn those parameters/weights with optimized hyperparameters. This cansave you a lot of time.
●If you have enough data, you can fine tune all the layers in your pretrained network but don'tra ndom initialize the parameters, leave the learned parameters as it is 

<br>

**Created by kishore**

---

<br>