/ [Home](../index.md) / [ML Archve](index.md)

# Handbook of Anomaly Detection: With Python Outlier Detection — (11) XGBOD

In Chapter 1, we talked about supervised learning can target better for known outliers, and unsupervised learning can explore new types of outliers. Can we take the advantage of both supervised and unsupervised learning? Specifically, since supervised learning achieves better precision in general, and the outlier scores from unsupervised learning identify outliers better, can we use the outlier scores from unsupervised learning as the features for supervised learning?

The above idea belongs to a larger concept called Representation learning. It is a machine-learning approach that discovers the data representations for features. In this chapter, I will explain representation learning, then introduce a supervised learning technique called XGBOD (Extreme Gradient Boosting Outlier Detection). I select XGBOD in this book so you can bridge to other representation learning variations such as BORE (Bagged Outlier Representation Ensemble).

(A) Representation Learning

Representation learning is a subject in machine learning that studies systematic ways to discover representations for raw data without any human intervention. The purpose of representation learning is to use machine learning algorithms to learn any normal and obscure patterns in data. The raw data can be represented by new features. Many dimension reduction techniques such as PCA and Autoencoder can provide this capability. In the literature representation learning [1] can be also called unsupervised feature engineering [2]. Let me humorously use a magnifying glass as an analogy to illustrate the idea. Imagine a magnifying glass scanning through the data. A magnifying glass may enlarge the normal patterns in the data, and another magnifying glass may enlarge the irregular data patterns. These different magnifying glasses are called features in data science. They produce new data to represent the original data.

The outlier scores of unsupervised learning are ways to represent the original data. Can they be used as the input features for a supervised learning model? That’s the BORE (Bagged Outlier Representation Ensemble) method by Micenková, et. al. [1]. If the outlier scores are used for supervised learning, they can provide a better predictive outcome for outliers.

(B) The Labelled Target Contains Different Types of Outliers

Before we talk about supervised learning, let’s understand the target. Outliers can come in different types. They will all be labeled as “1” in a binary classification model. Let me bring a more practical example to explain this point. Consider different types of auto fraud. One type of anomaly can be staged accidents to claim compensation. Another type of anomaly can be a false statement by a body shop. Or another type can be a claim about a stolen car. A financially distressed car owner may dispose of his vehicle by burning it, selling it, or even dumping it in a lake, and then claiming it was stolen. All the above are different types of outliers that come from their specific distributions. If the claims are plotted as data points on a 2D graph, they may be the outliers O1, O2, a1, and a2 in Figure (A). This prediction problem still can be formulated as a binary classification problem in which all types of anomalies are “1” and the rest as 0.

(C) XGBOD

The supervised learning methods can be any classification model such as the Logistic Regression used by BORE [1]. Continuing on the representation learning approach, Zhao & Maciej K. Hryniewicki (2019) [3] propose an XGBoost-based model called XGBOD (Extreme Gradient Boosting Outlier Detection). XGBoost has been documented to handle imbalanced data better than other ensemble methods [5]. This is an attraction for extremely imbalanced targets as mentioned above. The XGBoosting (EXtreme Gradient Boosting) algorithm by Chen and Guestrin [4] is a well-known implementation of the gradient-boosted trees algorithm. XGBoost mitigates overfitting with its built-in regularization formalization in its loss function. Its parallel processing and optimized computation also are attractive to many data scientists.

XGBOD has three steps. First, it applies unsupervised learning to create new features, called Transformed Outlier Scores (TOS). Second, it concatenates the new features with the original features, then applies Pearson’s correlation coefficients to keep the useful features. Third, it trains an XGBoost classifier. The use of XGBoost allows feature pruning and can provide feature importance rankings.

For the generation of the TOS, XGBOD includes KNN, AvgKNN, LOF, iForest, HBOS, and OCSVM as the default methods unless otherwise specified. The list of methods is extensive though not exhaustive. Multiple TOS will be generated by the models with different hyper-parameters. Below are the default models and their ranges of hyper-parameters.

- KNN, AvgKNN, LOF: The pre-defined range of n_neighbors for KNN, AvgKNN, and LOF is [1, 3, 5, 10, 20, 30, 40, 50]
- iForest: The pre-defined range for number of estimators is [10, 20, 50, 70, 100, 150, 200]
- HBOS: The pre-defined range of bins is [5, 10, 15, 20, 25, 30, 50]
- OCSVM: The pre-defined range of nu is [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99]

(D) Modeling procedure

In the unsupervised learning methods in this book, I apply a Step 1–2–3 modeling procedure for (1) Model development, (2) Threshold determination, and (3) Profiles of the normal and abnormal groups. However, we can skip (2) because the target is known in XGBOD.

The descriptive statistics (such as the means and standard deviations) of the features between the two groups are important to communicate the soundness of the model. If the results are counter-intuitive, you shall investigate, modify, or drop the feature and iterate the model until all features have sound interpretations.

(D.1) Step 1 — Build your Model

I generate six variables and 500 observations for the training and test data separately. The percentage of outliers is set by the contamination rate at 5%.

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pyod.utils.data import generate_data
contamination = 0.05 # percentage of outliers
n_train = 500       # number of training points
n_test = 500        # number of testing points
n_features = 6      # number of features
X_train, X_test, y_train, y_test = generate_data(
    n_train=n_train, 
    n_test=n_test, 
    n_features= n_features, 
    contamination=contamination, 
    random_state=123)

# Make the 2d numpy array a pandas dataframe for each manipulation 
X_train_pd = pd.DataFrame(X_train)
    
# Plot
plt.scatter(X_train_pd[0], X_train_pd[1], c=y_train, alpha=0.8)
plt.title('Scatter plot')
plt.xlabel('x0')
plt.ylabel('x1')
plt.show()
```

Figure (D.1) plots a scatter plot against the first two variables. The yellow dots are the outliers and the purple dots are the normal data points.

I apply the function decision_functions() to assign the anomaly score for each observation in “X_train” and “X_test”.

```
from pyod.models.xgbod import XGBOD
xgbod = XGBOD(n_components=4,random_state=100) 
xgbod.fit(X_train,y_train)

# get the prediction labels and outlier scores of the training data
y_train_pred = xgbod.labels_  # binary labels (0: inliers, 1: outliers)
y_train_scores = xgbod.decision_scores_  # raw outlier scores
y_train_scores = xgbod.decision_function(X_train)
# get the prediction on the test data
y_test_pred = xgbod.predict(X_test)  # outlier labels (0 or 1)
y_test_scores = xgbod.decision_function(X_test)  # outlier scores

def count_stat(vector):
    # Because it is '0' and '1', we can run a count statistic. 
    unique, counts = np.unique(vector, return_counts=True)
    return dict(zip(unique, counts))

print("The training data:", count_stat(y_train_pred))
print("The test data:", count_stat(y_test_pred))
```

Because we have the ground truth for the test data, we can verify the predictability of the model. The confusion matrix is more satisfactory. The model identifies the 25 data points correctly and only missed one data point.

```
Actual_pred = pd.DataFrame({'Actual': y_test, 'Pred': y_test_pred})
pd.crosstab(Actual_pred['Actual'],Actual_pred['Pred'])
```

Representation learning is essential in XGBOD. It applies unsupervised learning to create the Transformed Outlier Scores (TOS). We can print out the setting of XGBOD to see the unsupervised learning settings by using .get_params(). The output includes the specifications for KNN, AvgKNN, LOF, IForest, HBOS, and OCSVM. Each of these unsupervised learning models has created the TOS as new features for XGBOD to add to the original features to build the model.

The output also prints out the hyper-parameters for the Extreme Gradient Boosting. For example, the learning rate for the XGBoost model is 0.1, the max depth of a tree is 3, and there are 100 boosted trees.

```
xgbod.get_params()
```

(D.2) Step 2 — Descriptive Statistics of the Normal and Abnormal Groups

The descriptive statistics (such as the means and standard deviations) of the features between the two groups are important to demonstrate the soundness of a model.

```
# Let's see how many '0's and '1's.
df_train = pd.DataFrame(X_train)
df_columns = df_train.columns
df_train['pred'] = y_train_pred
df_train['Group'] = np.where(df_train['pred']==1, 'Outlier','Normal')

# Now let's show the summary statistics:
cnt = df_train.groupby('Group')['pred'].count().reset_index().rename(columns={'pred':'Count'})
cnt['Count %'] = (cnt['Count'] / cnt['Count'].sum()) * 100 # The count and count %
stat = df_train.groupby('Group').mean().reset_index() # The avg.
cnt.merge(stat, left_on='Group',right_on='Group') # Put the count and the avg. together
```

The above table shows the count and count percentage of the normal and outlier groups. You are reminded to label the features with their feature names for an effective presentation. The table tells us several important results:

- The size of the outlier group: The outlier group is about 10%. Remember the size of the outlier group is determined by the threshold. The size will shrink if you choose a higher value for the threshold.

- The feature statistics in each group: The table shows the outlier group has smaller values for Feature ‘0’ to ‘5’ than those of the normal group. In a business application, you probably expect the feature values in the outlier group to be higher or lower than those of the normal group. Therefore the feature statistics help to make sense of the model results.


(E) Summary

- Representation learning studies systematic ways to discover representations for raw data without any human intervention.
    
- XGBOD (Extreme Gradient Boosting Outlier Detection) applies different unsupervised outlier detection to create new features called Transformed Outlier Scores (TOS). It uses Pearson’s correlation coefficients to keep the useful features.
    
- The default unsupervised learning models for representation learning include KNN, AvgKNN, LOF, iForest, HBOS, and OCSVM.
    
- XGBOD adds the TOS to the original features to build the model.


(F) Python Notebook: Click here for the notebook.

References

    [1] B. Micenková, B. McWilliams, and I. Assent, “Learning Representations for Outlier Detection on a Budget.” 29-Jul-2015.
    [2] C. C. Aggarwal and S. Sathe, “Outlier ensembles: An introduction.” 2017.
    [3] Zhao, Y. & Hryniewicki, M. K. (2018). XGBOD: Improving Supervised Outlier Detection with Unsupervised Representation Learning. IJCNN (p./pp. 1–8), : IEEE. ISBN: 978–1–5090–6014–6
    [4] Chen, T., & Guestrin, C. (2016, August). Xgboost: A scalable tree-boosting system. In Proceedings of the 22nd ACM sigkdd international conference on knowledge discovery and data mining (pp. 785–794).
    [5] N. Moniz and P. Branco, “Evaluation of Ensemble Methods in Imbalanced Regression Tasks,” Proc. First Int. Work. Learn. with Imbalanced Domains Theory Appl., vol. 74, pp. 129–140, 2017.


For easy navigation to chapters, I list the chapters at the end.

    Chapter 1 — Introduction
    Chapter 2 — Histogram-Based Outlier Score (HBOS)
    Chapter 3 — Empirical Cumulative Outlier Detection (ECOD)
    Chapter 4 — Isolation Forest (IForest)
    Chapter 5 — Principal Component Analysis (PCA)
    Chapter 6 — One-Class Support Vector Machine (OCSVM)
    Chapter 7 — Gaussian Mixed Models (GMM)
    Chapter 8 — K-nearest Neighbors (KNN)
    Chapter 9 — Local Outlier Factor (LOF)
    Chapter 10 — Clustering-Based Local Outlier Factor (CBLOF)
    Chapter 11 — Extreme Boosting-Based Outlier Detection (XGBOD)
    Chapter 12 — Autoencoders
    Chapter 13 — Under-sampling for Extremely Imbalanced Data
    Chapter 14 — Over-sampling for Extremely Imbalanced Data



https://dataman-ai.medium.com/handbook-of-anomaly-detection-with-python-outlier-detection-11-xgbod-8ce51ebf81b0
