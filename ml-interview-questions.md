/ [Home](index.md)

# ML Interview Questions


1) What is Machine learning?

    - Machine learning is a branch of computer science which deals with system programming in order to automatically learn and improve with experience. For example: Robots are programed so that they can perform the task based on data they gather from sensors. It automatically learns programs from data.

2) Mention the difference between Data Mining and Machine learning?

   - Machine learning relates with the study, design and development of the algorithms that give computers the capability to learn without being explicitly programmed. While, data mining can be defined as the process in which the unstructured data tries to extract knowledge or unknown interesting patterns. During this process machine, learning algorithms are used.


3) What is ‘Overfitting’ in Machine learning?

     - In machine learning, when a statistical model describes random error or noise instead of underlying relationship ‘overfitting’ occurs. When a model is excessively complex, overfitting is normally observed, because of having too many parameters with respect to the number of training data types. The model exhibits poor performance which has been overfit.

4) Why overfitting happens?

     - The possibility of overfitting exists as the criteria used for training the model is not the same as the criteria used to judge the efficacy of a model.

5) What is inductive machine learning?

     - The inductive machine learning involves the process of learning by examples, where a system, from a set of observed instances tries to induce a general rule.

6) What are the five popular algorithms of Machine Learning?

      * Decision Trees
      * Neural Networks (back propagation)
      * Probabilistic networks
      * Nearest Neighbor
      * Support vector machines

7) What are the different Algorithm techniques in Machine Learning?

      * Supervised Learning
      * Unsupervised Learning
      * Semi-supervised Learning
      * Reinforcement Learning
      * Transduction
      * Learning to Learn

8) What are the three stages to build the hypotheses or model in machine learning?

    * Model building
    * Model testing
    * Applying the model

9) What is the standard approach to supervised learning?

    - The standard approach to supervised learning is to split the set of example into the training set and the test.

10) What is ‘Training set’ and ‘Test set’?

    - In various areas of information science like machine learning, a set of data is used to discover the potentially predictive relationship known as ‘Training Set’. Training set is an examples given to the learner, while Test set is used to test the accuracy of the hypotheses generated by the learner, and it is the set of example held back from the learner. Training set are distinct from Test set.

11) List down various approaches for machine learning?

    - The different approaches in Machine Learning are
        1. Concept Vs Classification Learning
        2. Symbolic Vs Statistical Learning
        3. Inductive Vs Analytical Learning

12) Explain what is the function of ‘Unsupervised Learning’?

    * Find clusters of the data
    * Find low-dimensional representations of the data
    * Find interesting directions in data
    * Interesting coordinates and correlations
    * Find novel observations/ database cleaning

13) Explain what is the function of ‘Supervised Learning’?

    * Classifications
    * Speech recognition
    * Regression
    * Predict time series
    * Annotate strings

14) What is algorithm independent machine learning?

    - Machine learning in where mathematical foundations is independent of any particular classifier or learning algorithm is referred as algorithm independent machine learning

15) What is classifier in machine learning?

    - A classifier in a Machine Learning is a system that inputs a vector of discrete or continuous feature values and outputs a single discrete value, the class.

16) What are the advantages of Naive Bayes?

    - In Naïve Bayes classifier will converge quicker than discriminative models like logistic regression, so you need less training data. The main advantage is that it can’t learn interactions between features.

17) In what areas Pattern Recognition is used?

     - Pattern Recognition can be used in

        * Computer Vision
        * Speech Recognition
        * ata Mining
        * Statistics
        * Informal Retrieval
        * Bio-Informatics


18) What is Genetic Programming?

     - Genetic programming is one of the two techniques used in machine learning. The model is based on the testing and selecting the best choice among a set of results.

19) What is Inductive Logic Programming in Machine Learning?

     - Inductive Logic Programming (ILP) is a subfield of machine learning which uses logical programming representing background knowledge and examples.

20) What is Model Selection in Machine Learning?

     - The process of selecting models among different mathematical models, which are used to describe the same data set is known as Model Selection. Model selection is applied to the fields of statistics, machine learning and data mining.

21) What are the two methods used for the calibration in Supervised Learning?

    - The two methods used for predicting good probabilities in Supervised Learning are

        * Platt Calibration
        * Isotonic Regression
These methods are designed for binary classification, and it is not trivial.

22) Which method is frequently used to prevent overfitting?

    - When there is sufficient data ‘Isotonic Regression’ is used to prevent an overfitting issue.

23) What is the difference between heuristic for rule learning and heuristics for decision trees?

    - The difference is that the heuristics for decision trees evaluate the average quality of a number of disjointed sets while rule learners only evaluate the quality of the set of instances that is covered with the candidate rule.

24) What is Perceptron in Machine Learning?

    - In Machine Learning, Perceptron is a supervised learning algorithm for binary classifiers where a binary classifier is a deciding function of whether an input represents a vector or a number.

25) What are Bayesian Networks (BN)?

    - Bayesian Network is used to represent the graphical model for probability relationship among a set of variables.

26) Why instance based learning algorithm sometimes referred as Lazy learning algorithm?

    - Instance based learning algorithm is also referred as Lazy learning algorithm as they delay the induction or generalization process until classification is performed.


27) What are the two classification methods that SVM ( Support Vector Machine) can handle?

    * Combining binary classifiers
    * Modifying binary to incorporate multiclass learning

28) What is ensemble learning?

     - To solve a particular computational program, multiple models such as classifiers or experts are strategically generated and combined. This process is known as ensemble learning.

29) Why ensemble learning is used?

    - Ensemble learning is used to improve the classification, prediction, function approximation etc of a model.

30) When to use ensemble learning?

     - Ensemble learning is used when you build component classifiers that are more accurate and independent from each other.

31) What are the two paradigms of ensemble methods?

    - The two paradigms of ensemble methods are

        * Sequential ensemble methods
        * Parallel ensemble methods


32) What is bias-variance decomposition of classification error in ensemble method?

    - The expected error of a learning algorithm can be decomposed into bias and variance. A bias term measures how closely the average classifier produced by the learning algorithm matches the target function. The variance term measures how much the learning algorithm’s prediction fluctuates for different training sets.


33) What is an Incremental Learning algorithm in ensemble?

    - Incremental learning method is the ability of an algorithm to learn from new data that may be available after classifier has already been generated from already available dataset.

34) What is PCA, KPCA and ICA used for?

    - PCA (Principal Components Analysis), KPCA ( Kernel based Principal Component Analysis) and ICA ( Independent Component Analysis) are important feature extraction techniques used for dimensionality reduction.

35) What is dimension reduction in Machine Learning?

    - In Machine Learning and statistics, dimension reduction is the process of reducing the number of random variables under considerations and can be divided into feature selection and feature extraction.

36) What are support vector machines?

    - Support vector machines are supervised learning algorithms used for classification and regression analysis.

37) What are the components of relational evaluation techniques?

    - The important components of relational evaluation techniques are

        * Data Acquisition
        * Ground Truth Acquisition
        * Cross Validation Technique
        * Query Type
        * Scoring Metric
        * Significance Test

38) What are the different methods for Sequential Supervised 
Learning?

    - The different methods to solve Sequential Supervised Learning problems are

        * Sliding-window methods
        * Recurrent sliding windows
        * Hidden Markow models
        * Maximum entropy Markow models
        * Conditional random fields
        * Graph transformer networks


39) What are the areas in robotics and information processing where sequential prediction problem arises?

    - The areas in robotics and information processing where sequential prediction problem arises are

        * Imitation Learning
        * Structured prediction
        * Model based reinforcement learning

40) What is batch statistical learning?

    - Statistical learning techniques allow learning a function or predictor from a set of observed data that can make predictions about unseen or future data. These techniques provide guarantees on the performance of the learned predictor on the future unseen data based on a statistical assumption on the data generating process.

41) What is PAC Learning?

    - PAC (Probably Approximately Correct) learning is a learning 
framework that has been introduced to analyze learning algorithms and their statistical efficiency.

42) What are the different categories you can categorized the sequence learning process?

     * Sequence prediction
     * Sequence generation
     * Sequence recognition
     * Sequential decision

43) What is sequence learning?

    - Sequence learning is a method of teaching and learning in a logical manner.

44) What are two techniques of Machine Learning?

    - The two techniques of Machine Learning are

        * Genetic Programming
        * Inductive Learning


45) What are the performance metrics that can be used to estimate the efficiency of a linear regression model?

    - The performance metric that is used in this case is:

        * Mean Squared Error
        * R2 score
        * Adjusted  R2 score
        * Mean Absolute score

46) What is the default method of splitting in decision 
trees?

     - The default method of splitting in decision trees is the Gini Index. Gini Index is the measure of impurity of a particular node.

     - This can be changed by making changes to classifier parameters. 

47) How is p-value useful?

    - The p-value gives the probability of the null hypothesis is true. It gives us the statistical significance of our results. In other words, p-value determines the confidence of a model in a particular output.

48) Can logistic regression be used for classes more than 2?

    - No, logistic regression cannot be used for classes more than 2 as it is a binary classifier. For multi-class classification algorithms like Decision Trees, Naïve Bayes’ Classifiers are better suited.

49) What are the hyperparameters of a logistic regression model?

     - Classifier penalty, classifier solver and classifier C are the trainable hyperparameters of a Logistic Regression Classifier. These can be specified exclusively with values in Grid Search to hyper tune a Logistic Classifier.

50) Name a few hyper-parameters of decision trees?

    - The most important features which one can tune in decision trees are:

        * Splitting criteria
        * Min_leaves
        * Min_samples
        * Max_depth

51) How to deal with multicollinearity?

    - Multi collinearity can be dealt with by the following steps:

    - Remove highly correlated predictors from the model.
Use Partial Least Squares Regression (PLS) or Principal Components Analysis

52) What is Heteroscedasticity?

     - It is a situation in which the variance of a variable is unequal across the range of values of the predictor variable.

     - It should be avoided in regression as it introduces unnecessary variance.  

53) How do you deal with the class imbalance in a classification problem?

    - Class imbalance can be dealt with in the following ways:

        * Using class weights
        * Using Sampling
        * Using SMOTE
        * Choosing loss functions like Focal Loss

54) What is the role of cross-validation?

     - Cross-validation is a technique which is used to increase the performance of a machine learning algorithm, where the machine is fed sampled data out of the same data for a few times. The sampling is done so that the dataset is broken into small parts of the equal number of rows, and a random part is chosen as the test set, while all other parts are chosen as train sets.


55) How would you handle missing or corrupted data in a dataset?

    - Dropping the rows or columns with the missing or corrupted dataset or replacing them entirely with a different value are two easy ways to handle such a situation. Methods like IsNull(), dropna(), and Fillna() help in accomplishing this task.

56)  State the applications of supervised machine learning in modern businesses?

     - Sentiment Analysis, Email Spam Detection, Fraud Detection, and Healthcare Diagnosis are some applications of Supervised Machine Learning. 

57) Explain the ensemble learning technique in machine learning?

    - Ensemble learning involves meta-algorithms combining various ML techniques into a single predictive model. The aim of doing that is stacking, bagging, or boosting. That is, to improve predictions, decrease variance, or decrease bias.

58) Differentiate between bagging and boosting?

    - Bagging is a way to merge the same type of predictions, whereas boosting refers to a method of merging different types of predictions. Bagging decreases variance, and boosting decreases bias, not vice versa.

59) How is KNN different from K-means clustering?

    - means is an unsupervised learning algorithm, whereas KNN is a supervised learning algorithm. K-means is mainly used for clustering problems, and the KNN algorithm is primarily used for classification and regression problems.

60) Which algorithms are most widely used in machine learning?

    - Linear regression, Decision tree, Logistic regression, KNN algorithm, K-means, SVM algorithm, Naive Bayes algorithm, and Random forest algorithm are some of the most widely used algorithms in Machine Learning.

61) How would you explain Machine Learning to a school-going kid?

    - Machine learning is an application of Artificial Intelligence where we give machines access to data and let them use that data to learn for themselves. Then, you can input new conditions and it will predict the outcome.It's basically getting a computer to perform a task without explicitly being programmed to do so.

62) How does Deep Learning differ from Machine Learning?

    - ML refers to an AI system that can self-learn based on the algorithm. Systems that get smarter and smarter over time without the human intervention is ML. Deep Learning is a machine learning applied to large data sets. Most AI work involves ML because intelligent behaviour requires considerable knowledge.

63) Explain Classification and Regression?

    - Classification is a process of categorizing a given set of data into classes, It can be performed on both structured or unstructured data. Regression in machine learning consists of mathematical methods that allow data scientists to predict a continuous outcome (y) based on the value of one or more predictor variables (x). Linear regression is probably the most popular form of regression analysis because of its ease-of-use in predicting and forecasting.

64) What do you understand by selection bias?

    - Selection bias is a kind of error that occurs when the researcher decides who is going to be studied. It is usually associated with the research where the selection of participants isn't random.

65) What do you understand by Precision and Recall?

    - Recall is the number of relevant documents retrieved by a search divided by the total number of the existing relevant documents, while precision is the number of relevant documents retrieved by a search divided by the total number of documents retrieved by that search.

66) What is a Confusion Matrix?

    - The confusion is a 26 by 26 matrix with the probability of each reaction to each stimulus. This explains the name and matches the use in machine learning today.

67) What is the difference between inductive and deductive learning?

    - The main difference between inductive and deductive reasoning is that inductive reasoning aims at developing a theory while deductive reasoning aims at testing an existing theory. Inductive reasoning moves from the specific observations to broad generalizations, and deductive reasoning the other way around.

68) How is KNN different from K-means clustering?

    - K-means is an unsupervised learning algorithm used for the clustering problem whereas KNN is a supervised learning algorithm used for classification and regression problem. This is the basic difference between K-means and KNN algorithm. It makes predictions by learning from the past available data.

69) What is ROC curve and what does it represent?

    - An ROC curve is a graph showing the performance of a classification model at all the classification thresholds. This curve plots two parameters: True Positive Rate. False Positive Rate.

70) What’s the difference between Type I and Type II error?

    - Type 1 error, in statistical hypothesis testing, is the error caused by rejecting a null hypothesis when it is true. Type II error is the error that occurs when the null hypothesis is accepted when it is not true. Type I error is equivalent to a false positive. Type II error is equivalent to a false negative.

71) s it better to have too many false positives or too many false negatives? Explain.

    - In medical testing, false negatives may provide a falsely reassuring message to patients and physicians that the disease is absent, when it is actually present. This sometimes leads to inappropriate or inadequate treatment of both the patient and their disease. So, it is desired to have too many false positive.

72) Which is more important to you – model accuracy or model performance?

    - The accuracy extremely critical, even if the models would take minutes or hours to make a prediction. Other applications require the real time performance, even if this comes at a cost of accuracy.

73)  What is the difference between Entropy and Information Gain?

     - The information gain is the amount of information gained about a random variable or signal from observing another random variable. Entropy is that the average rate at which information is produced by a stochastic source of data, Or, it is a measure of the uncertainty associated with a random variable.

74) Explain Ensemble learning technique in Machine Learning?

    - Ensemble methods are meta-algorithms that combine several machine learning techniques into the one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).

75)  What are collinearity and multicollinearity?

     - Collinearity is a linear association between the two predictors. Multicollinearity is a situation where two or more predictors are highly linearly related. In general, an absolute correlation coefficient of >0.7 among two or more predictors indicates the presence of multicollinearity.

76) How will you explain the Fourier Transformation in Machine Learning?

    - A Fourier Transformation is the generic method that helps in decomposing functions into a series of symmetric functions. It helps you in finding the set of cycle speeds, phases, and amplitude to match the particular time signal. It has the capability to convert the signal into frequency domain like sensor data or more.

77) How will you differentiate the generic model from the discriminative model?

    - A generic model will explain the multiple categories of data while the discriminative model simply tells the difference between data categories. They are used in classification tasks and need to understand deeply before you actually implement them.

78) List different types of cloud services?

    - Various types of cloud services are:

        * Software as a Service (SaaS)
        * Data as a Service (DaaS)
        * Platform as a Service (PaaS)
        * Infrastructure as a Service (IaaS).

79) Explain the term Q-Learning?

    -  Q-learning is a popular reinforcement learning algorithm. In this, the agent tries to learn the optimal policies that can provide the best actions to maximize the environment’s rewards. The agent learns these optimal policies from past experiences.

80) What is Rectified Linear Unit (ReLU) in Machine learning?

    - An activation function with the following rules:

        If input is negative or zero, output is 0.

        If input is positive, output is equal to input.

81) What is AdaGrad algorithm?

    - A sophisticated gradient descent algorithm that rescales the gradients of each parameter, effectively giving each parameter an independent learning rate.

82)  What is backpropagation?

        - The primary algorithm for performing gradient descent on neural networks. First, the output values of each node are calculated (and cached) in a forward pass. Then, the partial derivative of the error with respect to each parameter is calculated in a backward pass through the graph.

            The Area Under the ROC curve is the probability that a classifier will be more confident that a randomly chosen positive example is actually positive than that a randomly chosen negative example is positive.

83) What do you understand about Variance Error in machine learning algorithms?

    - Variance error is common in machine learning when the algorithm is highly complex and difficult to understand as well. It may lead to a high degree of variation to your training data that can lead the model to overfit the data. Also, there could be so much noise for the training data that is not necessary in case of the test data.

84)  How will you differentiate the L1 and L2 regularization?

     -  L2 regularization tends to spread error among multiple terms while L! is more specific to binary variables where either 0 or 1 is assigned based on requirements. L1 tends to set a Laplacian prior on terms, but L2 tends to set a Gaussian prior on terms.

85) What is your favorite algorithm? Explain in less than a minute based on your past experiences.

    - The answer to this question will vary based on the projects you worked on earlier. Also, which algorithm assured better outcomes as compared to others.

86) What is cross-validation in Machine Learning?

    - The cross-validation method in Machine Learning allows a system to enhance the performance of the given Machine Learning algorithm to which you feed various sample data from the dataset. This sampling process is done to break the dataset into smaller parts that have the same number of rows, out of which a random part is selected as a test set, and the rest of the parts are kept as train sets. Cross-validation includes the following techniques:

        * Holdout method
        * K-fold cross-validation
        * Stratified k-fold cross-validation 
        * Leave p-out cross-validation

87)  What seems more important is either model accuracy or performance of a model?

     - Well, model accuracy is just a subset of the model performance parameter. For a model who is performing excellent, there are chances of more accuracy than others.

88) Differentiate Sigmoid and Softmax functions?

    - The sigmoid function is used for binary classification and the probabilities sum required to be 1. Whereas, Softmax function is used for multi-classification and its probability sum will be 1.

89) What is PCA? When do you use it?

    - Principal component analysis (PCA) is most commonly used for dimension reduction.

        In this case, PCA measures the variation in each variable (or column in the table). If there is little variation, it throws the variable out, as illustrated in the figure below:


        Principal component analysis (PCA)
        Thus making the dataset easier to visualize. PCA is used in finance, neuroscience, and pharmacology.

        It is very useful as a preprocessing step, especially when there are linear correlations between features.

90)  What are Different Kernels in SVM?

     - There are six types of kernels in SVM:

        * Linear kernel - used when data is linearly separable. 
        * Polynomial kernel - When you have discrete data that has no natural notion of smoothness.
        * Radial basis kernel - Create a decision boundary able to do a much better job of separating two classes than the linear kernel.
        * Sigmoid kernel - used as an activation function for neural networks.

91) What are Loss Function and Cost Functions? Explain the key Difference Between them?

    - When calculating loss we consider only a single data point, then we use the term loss function.

        Whereas, when calculating the sum of error for multiple data then we use the cost function. There is no major difference.

        In other words, the loss function is to capture the difference between the actual and predicted values for a single record whereas cost functions aggregate the difference for the entire training dataset.

        The Most commonly used loss functions are Mean-squared error and Hinge loss.

        Mean-Squared Error(MSE): In simple words, we can say how our model predicted values against the actual values.

        MSE = √(predicted value - actual value)2
        Hinge loss: It is used to train the machine learning classifier, which is

        L(y) = max(0,1- yy)

        Where y = -1 or 1 indicating two classes and y represents the output form of the classifier. The most common cost function represents the total cost as the sum of the fixed costs and the variable costs in the equation y = mx + b

92) What is Ensemble learning?

    - Ensemble learning is a method that combines multiple machine learning models to create more powerful models.

        There are many reasons for a model to be different. Few reasons are:

        Different Population
        Different Hypothesis
        Different modeling techniques
        When working with the model’s training and testing data, we will experience an error. This error might be bias, variance, and irreducible error.

        Now the model should always have a balance between bias and variance, which we call a bias-variance trade-off.

        This ensemble learning is a way to perform this trade-off.

        There are many ensemble techniques available but when aggregating multiple models there are two general methods:

        Bagging, a native method: take the training set and generate new training sets off of it.
        Boosting, a more elegant method: similar to bagging, boosting is used to optimize the best weighting scheme for a training set.

93) How do you make sure which Machine Learning Algorithm to use?

    - It completely depends on the dataset we have. If the data is discrete we use SVM. If the dataset is continuous we use linear regression.

        So there is no specific way that lets us know which ML algorithm to use, it all depends on the exploratory data analysis (EDA).

        EDA is like “interviewing” the dataset; As part of our interview we do the following:

        Classify our variables as continuous, categorical, and so forth. 
        Summarize our variables using descriptive statistics. 
        Visualize our variables using charts.
        Based on the above observations select one best-fit algorithm for a particular dataset.

94) How to Handle Outlier Values?

    - An Outlier is an observation in the dataset that is far away from other observations in the dataset. Tools used to discover outliers are

        Box plot
        Z-score
        Scatter plot, etc.
        Typically, we need to follow three simple strategies to handle outliers:

        We can drop them. 
        We can mark them as outliers and include them as a feature. 
        Likewise, we can transform the feature to reduce the effect of the outlier.

95) What is a Random Forest? How does it work?

    - Random forest is a versatile machine learning method capable of performing both regression and classification tasks.

        Like bagging and boosting, random forest works by combining a set of other tree models. Random forest builds a tree from a random sample of the columns in the test data.

        Here’s are the steps how a random forest creates the trees:

        Take a sample size from the training data.
        Begin with a single node.
        Run the following algorithm, from the start node:
        If the number of observations is less than node size then stop.
        Select random variables.
        Find the variable that does the “best” job of splitting the observations.
        Split the observations into two nodes.
        Call step `a` on each of these nodes.

96) What is Collaborative Filtering? And Content-Based Filtering?

    - Collaborative filtering is a proven technique for personalized content recommendations. Collaborative filtering is a type of recommendation system that predicts new content by matching the interests of the individual user with the preferences of many users.

        Content-based recommender systems are focused only on the preferences of the user. New recommendations are made to the user from similar content according to the user’s previous choices.

97) How do check the Normality of a dataset?

    - Visually, we can use plots. A few of the normality checks are as follows:

        * Shapiro-Wilk Test
        * Anderson-Darling Test
        * Martinez-Iglewicz Test
        * Kolmogorov-Smirnov Test
        * D’Agostino Skewness Test

98) Can logistic regression use for more than 2 classes?

    - No, by default logistic regression is a binary classifier, so it cannot be applied to more than 2 classes. However, it can be extended for solving multi-class classification problems (multinomial logistic regression)

99) What is P-value?

    - P-values are used to make a decision about a hypothesis test. P-value is the minimum significant level at which you can reject the null hypothesis. The lower the p-value, the more likely you reject the null hypothesis.

100) What are Parametric and Non-Parametric Models?

     - Parametric models will have limited parameters and to predict new data, you only need to know the parameter of the model.

        Non-Parametric models have no limits in taking a number of parameters, allowing for more flexibility and to predict new data. You need to know the state of the data and model parameters.

101) What is Reinforcement Learning? 

     - Reinforcement learning is different from the other types of learning like supervised and unsupervised. In reinforcement learning, we are given neither data nor labels. Our learning is based on the rewards given to the agent by the environment.

102) What Are the Applications of Supervised Machine Learning in Modern Businesses?

        - Applications of supervised machine learning include:

        Email Spam Detection
        Here we train the model using historical data that consists of emails categorized as spam or not spam. This labeled information is fed as input to the model.
        Healthcare Diagnosis
        By providing images regarding a disease, a model can be trained to detect if a person is suffering from the disease or not.
        Sentiment Analysis
        This refers to the process of using algorithms to mine documents and determine whether they’re positive, neutral, or negative in sentiment. 
        Fraud Detection
        By training the model to identify suspicious patterns, we can detect instances of possible fraud.

103) What is Semi-supervised Machine Learning?

        - Supervised learning uses data that is completely labeled, whereas unsupervised learning uses no training data.

             In the case of semi-supervised learning, the training data contains a small amount of labeled data and a large amount of unlabeled data.

104) What Are Unsupervised Machine Learning Techniques? 

        - There are two techniques used in unsupervised learning: clustering and association.

            * Clustering
            Clustering problems involve data to be divided into subsets. These subsets, also called clusters, contain data that are similar to each other. Different clusters reveal different details about the objects, unlike classification or regression.

            * Association
            In an association problem, we identify patterns of associations between different variables or items.

            For example, an e-commerce website can suggest other items for you to buy, based on the prior purchases that you have made, spending habits, items in your wishlist, other customers’ purchase habits, and so on.

105) What Is ‘naive’ in the Naive Bayes Classifier?

        - The classifier is called ‘naive’ because it makes assumptions that may or may not turn out to be correct. 

            The algorithm assumes that the presence of one feature of a class is not related to the presence of any other feature (absolute independence of features), given the class variable.

            For instance, a fruit may be considered to be a cherry if it is red in color and round in shape, regardless of other features. This assumption may or may not be right (as an apple also matches the description).

106) Explain How a System Can Play a Game of Chess Using Reinforcement Learning?

        - Reinforcement learning has an environment and an agent. The agent performs some actions to achieve a specific goal. Every time the agent performs a task that is taking it towards the goal, it is rewarded. And, every time it takes a step that goes against that goal or in the reverse direction, it is penalized. 

            Earlier, chess programs had to determine the best moves after much research on numerous factors. Building a machine designed to play such games would require many rules to be specified. 

            With reinforced learning, we don’t have to deal with this problem as the learning agent learns by playing the game. It will make a move (decision), check if it’s the right move (feedback), and keep the outcomes in memory for the next step it takes (learning). There is a reward for every correct decision the system takes and punishment for the wrong one. 

107) How Will You Know Which Machine Learning Algorithm to Choose for Your Classification Problem?

        - While there is no fixed rule to choose an algorithm for a classification problem, you can follow these guidelines:

            If accuracy is a concern, test different algorithms and cross-validate them
            If the training dataset is small, use models that have low variance and high bias
            If the training dataset is large, use models that have high variance and little bias

108) When Will You Use Classification over Regression?

        - Classification is used when your target is categorical, while regression is used when your target variable is continuous. Both classification and regression belong to the category of supervised machine learning algorithms. 

            Examples of classification problems include:

            Predicting yes or no
            Estimating gender
            Breed of an animal
            Type of color
            Examples of regression problems include:

            Estimating sales and price of a product
            Predicting the score of a team
            Predicting the amount of rainfall

109) How Do You Design an Email Spam Filter?

        - Building a spam filter involves the following process:

            The email spam filter will be fed with thousands of emails 
            Each of these emails already has a label: ‘spam’ or ‘not spam.
            The supervised machine learning algorithm will then determine which type of emails are being marked as spam based on spam words like the lottery, free offer, no money, full refund, etc.
            The next time an email is about to hit your inbox, the spam filter will use statistical analysis and algorithms like Decision Trees and SVM to determine how likely the email is spam
            If the likelihood is high, it will label it as spam, and the email won’t hit your inbox
            Based on the accuracy of each model, we will use the algorithm with the highest accuracy after testing all the models

110) What is Pruning in Decision Trees, and How Is It Done?

        - Pruning is a technique in machine learning that reduces the size of decision trees. It reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting. 

        Pruning can occur in:

        Top-down fashion. It will traverse nodes and trim subtrees starting at the root
        Bottom-up fashion. It will begin at the leaf nodes
        There is a popular pruning algorithm called reduced error pruning, in which:

        Starting at the leaves, each node is replaced with its most popular class
        If the prediction accuracy is not affected, the change is kept
        There is an advantage of simplicity and speed

111) What is Kernel SVM?

     - Kernel SVM is the abbreviated version of the kernel support vector machine. Kernel methods are a class of algorithms for pattern analysis, and the most common one is the kernel SVM.

112)  What Are Some Methods of Reducing Dimensionality?

         - You can reduce dimensionality by combining features with feature engineering, removing collinear features, or using algorithmic dimensionality reduction.

       Now that you have gone through these machine learning interview questions, you must have got an idea of your strengths and weaknesses in this domain.

113)  What is ‘Training set’ and ‘Test set’?

         - In various areas of information science like machine learning, a set of data is used to discover the potentially predictive relationship known as ‘Training Set’. Training set is an examples given to the learner, while Test set is used to test the accuracy of the hypotheses generated by the learner, and it is the set of example held back from the learner. Training set are distinct from Test set.

114) What is the difference between heuristic for rule learning and heuristics for decision trees?

     - The difference is that the heuristics for decision trees evaluate the average quality of a number of disjointed sets while rule learners only evaluate the quality of the set of instances that is covered with the candidate rule.

115) What is Perceptron in Machine Learning?

     - In Machine Learning, Perceptron is a supervised learning algorithm for binary classifiers where a binary classifier is a deciding function of whether an input represents a vector or a number.

116) What are Bayesian Networks (BN)?

        - Bayesian Network is used to represent the graphical model for probability relationship among a set of variables.

117) What is a voting model?

     - A voting model is an ensemble model which combines several classifiers but to produce the final result, in case of a classification-based model, takes into account, the classification of a certain data point of all the models and picks the most vouched/voted/generated option from all the given classes in the target column.

118) How to deal with very few data samples? Is it possible to make a model out of it?

     - If very few data samples are there, we can make use of oversampling to produce new data points. In this way, we can have new data points.

119) What are the hyperparameters of an SVM?

     - The gamma value, c value and the type of kernel are the hyperparameters of an SVM model.

120) What is Pandas Profiling?

     - Pandas profiling is a step to find the effective number of usable data. It gives us the statistics of NULL values and the usable values and thus makes variable selection and data selection for building models in the preprocessing phase very effective.

121) What impact does correlation have on PCA?

     - If data is correlated PCA does not work well. Because of the correlation of variables the effective variance of variables decreases. Hence correlated data when used for PCA does not work well.

122) How is PCA different from LDA?

      - PCA is unsupervised. LDA is unsupervised.

     - PCA takes into consideration the variance. LDA takes into account the distribution of classes.

123) What distance metrics can be used in KNN?

     - Following distance metrics can be used in KNN.

        * Manhattan
        * Minkowski
        * Tanimoto
        * Jaccard
        * Mahalanobis

124) Which metrics can be used to measure correlation of categorical data?

     - Chi square test can be used for doing so. It gives the measure of correlation between categorical predictors.

125) Which algorithm can be used in value imputation in both categorical and continuous categories of data?

     - KNN is the only algorithm that can be used for imputation of both categorical and continuous variables.

126) When should ridge regression be preferred over lasso?

     - We should use ridge regression when we want to use all predictors and not remove any as it reduces the coefficient values but does not nullify them.

127) Which algorithms can be used for important variable selection?

     - Random Forest, Xgboost and plot variable importance charts can be used for variable selection.

128) What ensemble technique is used by Random forests?

     - Bagging is the technique used by Random Forests. Random forests are a collection of trees which work on sampled data from the original dataset with the final prediction being a voted average of all trees.

129) What ensemble technique is used by gradient boosting trees?

     - Boosting is the technique used by GBM.

130) What is a good metric for measuring the level of multicollinearity?

     - VIF or 1/tolerance is a good measure of measuring multicollinearity in models. VIF is the percentage of the variance of a predictor which remains unaffected by other predictors. So higher the VIF value, greater is the multicollinearity amongst the predictors.

     - A rule of thumb for interpreting the variance inflation factor:

        1 = not correlated.
        Between 1 and 5 = moderately correlated.
        Greater than 5 = highly correlated.

131) When can be a categorical value treated as a continuous variable and what effect does it have when done so?

     - A categorical predictor can be treated as a continuous one when the nature of data points it represents is ordinal. If the predictor variable is having ordinal data then it can be treated as continuous and its inclusion in the model increases the performance of the model.

132) What is the role of maximum likelihood in logistic regression?

     - Maximum likelihood equation helps in estimation of most probable values of the estimator’s predictor variable coefficients which produces results which are the most likely or most probable and are quite close to the truth values.
     
133) Which distance do we measure in the case of KNN?

     - The hamming distance is measured in case of KNN for the determination of nearest neighbours. Kmeans uses euclidean distance.

134) What is a pipeline?

     - A pipeline is a sophisticated way of writing software such that each intended action while building a model can be serialized and the process calls the individual functions for the individual tasks. The tasks are carried out in sequence for a given sequence of data points and the entire process can be run onto n threads by use of composite estimators in scikit learn.

135) Which sampling technique is most suitable when working with time-series data?

     - We can use a custom iterative sampling such that we continuously add samples to the train set. We only should keep in mind that the sample used for validation should be added to the next train sets and a new sample is used for validation.

136) What are the benefits of pruning?

   - Pruning helps in the following:

        * Reduces overfitting
        * Shortens the size of the tree
        * Reduces complexity of the model
        * Increases bias

137) What is normal distribution?

     - The distribution having the below properties is called normal distribution. 

        * The mean, mode and median are all equal.
        * The curve is symmetric at the center (i.e. around the mean, μ).
        * Exactly half of the values are to the left of center and exactly  half the values are to the right.
        * The total area under the curve is 1.

138) What is the 68 per cent rule in normal distribution?

     - The normal distribution is a bell-shaped curve. Most of the data points are around the median. Hence approximately 68 per cent of the data is around the median. Since there is no skewness and its bell-shaped. 

139) What is a chi-square test?

     - A chi-square determines if a sample data matches a population. 

     - A chi-square test for independence compares two variables in a contingency table to see if they are related.

     - A very small chi-square test statistics implies observed data fits the expected data extremely well. 

140) What is a random variable?

     - A Random Variable is a set of possible values from a random experiment. Example: Tossing a coin: we could get Heads or Tails. Rolling of a dice: we get 6 values

141) What is the degree of freedom?

     - It is the number of independent values or quantities which can be assigned to a statistical distribution. It is used in Hypothesis testing and chi-square test.

142) What is a false positive?

     - It is a test result which wrongly indicates that a particular condition or attribute is present.

     - Example – “Stress testing, a routine diagnostic tool used in detecting heart disease, results in a significant number of false positives in women”

143) What is a false negative?

     - A test result which wrongly indicates that a particular condition or attribute is absent.

     - Example – “it’s possible to have a false negative—the test says you aren’t pregnant when you are”

144) What is the error term composed of in regression?

     - Error is a sum of bias error+variance error+ irreducible error in regression. Bias and variance error can be reduced but not the irreducible error.

145) Which performance metric is better R2 or adjusted R2?

     - Adjusted R2 because the performance of predictors impacts it. R2 is independent of predictors and shows performance improvement through increase if the number of predictors is increased.

146) What’s the difference between Type I and Type II error?

     - Type I and Type II error in machine learning refers to false values. Type I is equivalent to a False positive while Type II is equivalent to a False negative. In Type I error, a hypothesis which ought to be accepted doesn’t get accepted. Similarly, for Type II error, the hypothesis gets rejected which should have been accepted in the first place.

147) What do you understand by L1 and L2 regularization?

     * L2 regularization: It tries to spread error among all the terms. L2 corresponds to a Gaussian prior.

     * L1 regularization: It is more binary/sparse, with many variables either being assigned a 1 or 0 in weighting. L1 corresponds to setting a Laplacean prior on the terms.

148) Which one is better, Naive Bayes Algorithm or Decision Trees?

     - Although it depends on the problem you are solving, but some general advantages are following:

        Naive Bayes:

        * Work well with small dataset compared to DT which need more data
        * Lesser overfitting
        * Smaller in size and faster in processing

        Decision Trees:

       * Decision Trees are very flexible, easy to understand, and easy to debug
       * No preprocessing or transformation of features required
       * Prone to overfitting but you can use pruning or Random forests to avoid that.

149) What do you mean by the ROC curve?

     - Receiver operating characteristics (ROC curve): ROC curve illustrates the diagnostic ability of a binary classifier. It is calculated/created by plotting True Positive against False Positive at various threshold settings. The performance metric of ROC curve is AUC (area under curve). Higher the area under the curve, better the prediction power of the model.

150) What do you mean by AUC curve?

     - AUC (area under curve). Higher the area under the curve, better the prediction power of the model.

151) What is the meaning of Overfitting?

     - Overfitting can be seen in machine learning when a statistical model describes random error or noise instead of the underlying relationship. Overfitting is usually observed when a model is excessively complex. It happens because of having too many parameters concerning the number of training data types. The model displays poor performance, which has been overfitted.

152)  What is the method to avoid overfitting?

      - Overfitting occurs when we have a small dataset, and a model is trying to learn from it. By using a large amount of data, overfitting can be avoided. But if we have a small database and are forced to build a model based on that, then we can use a technique known as cross-validation. In this method, a model is usually given a dataset of a known data on which training data set is run and dataset of unknown data against which the model is tested. The primary aim of cross-validation is to define a dataset to "test" the model in the training phase. If there is sufficient data, 'Isotonic Regression' is used to prevent overfitting.

153) How is KNN different from k-means?

     - KNN or K nearest neighbors is a supervised algorithm which is used for classification purpose. In KNN, a test sample is given as the class of the majority of its nearest neighbors. On the other side, K-means is an unsupervised algorithm which is mainly used for clustering. In k-means clustering, it needs a set of unlabeled points and a threshold only. The algorithm further takes unlabeled data and learns how to cluster it into groups by computing the mean of the distance between different unlabeled points.

154) What are the five popular algorithms we use in Machine Learning?

     - Five popular algorithms are:

       * Decision Trees
       * Probabilistic Networks
       * Neural Networks
       * Support Vector Machines
       * Nearest Neighbor

155) What is a model selection in Machine Learning?

     - The process of choosing models among diverse mathematical models, which are used to define the same data is known as Model Selection. Model learning is applied to the fields of statistics, data mining, and machine learning.

156) What do you understand by ILP?

     - ILP stands for Inductive Logic Programming. It is a part of machine learning which uses logic programming. It aims at searching patterns in data which can be used to build predictive models. In this process, the logic programs are assumed as a hypothesis.

157) What are the functions of Supervised Learning?

     - Classification
        * Speech Recognition
        * Regression
        * Predict Time Series
        * Annotate Strings

158) What are the functions of Unsupervised Learning?

        - Finding clusters of the data
            * Finding low-dimensional representations of the data
            * Finding interesting directions in data
            * Finding novel observations/ database cleaning
            * Finding interesting coordinates and correlations

159) What do you mean by Genetic Programming?

     - Genetic Programming (GP) is almost similar to an Evolutionary Algorithm, a subset of machine learning. Genetic programming software systems implement an algorithm that uses random mutation, a fitness function, crossover, and multiple generations of evolution to resolve a user-defined task. The genetic programming model is based on testing and choosing the best option among a set of results.

160) Explain True Positive, True Negative, False Positive, and False Negative in Confusion Matrix with an example?

     - True Positive
When a model correctly predicts the positive class, it is said to be a true positive.
For example, Umpire gives a Batsman NOT OUT when he is NOT OUT.
     - True Negative
When a model correctly predicts the negative class, it is said to be a true negative.
For example, Umpire gives a Batsman OUT when he is OUT.
      - False Positive
When a model incorrectly predicts the positive class, it is said to be a false positive. It is also known as 'Type I' error.
For example, Umpire gives a Batsman NOT OUT when he is OUT.
       - False Negative
When a model incorrectly predicts the negative class, it is said to be a false negative. It is also known as 'Type II' error.
For example, Umpire gives a Batsman OUT when he is NOT OUT.

161) Which are the two components of Bayesian logic program?

     - A Bayesian logic program consists of two components:

        Logical
        It contains a set of Bayesian Clauses, which capture the qualitative structure of the domain.
        Quantitative
        It is used to encode quantitative information about the domain.

162) Why instance-based learning algorithm sometimes referred to as Lazy learning algorithm?

     - In machine learning, lazy learning can be described as a method where induction and generalization processes are delayed until classification is performed. Because of the same property, an instance-based learning algorithm is sometimes called lazy learning algorithm.

163) What is Hypothesis in Machine Learning?

     - Machine Learning allows the use of available dataset to understand a specific function that maps input to output in the best possible way. This problem is known as function approximation. Here, approximation needs to be used for the unknown target function that maps all plausible observations based on the given problem in the best manner. Hypothesis in Machine learning is a model that helps in approximating the target function and performing the necessary input-to-output mappings. The choice and configuration of algorithms allow defining the space of plausible hypotheses that may be represented by a model.

164) What is Entropy?

     - Entropy in Machine Learning measures the randomness in the data that needs to be processed. The more entropy in the given data, the more difficult it becomes to draw any useful conclusion from the data. For example, let us take the flipping of a coin. The result of this act is random as it does not favor heads or tails. Here, the result for any number of tosses cannot be predicted easily as there is no definite relationship between the action of flipping and the possible outcomes.

165) What is Epoch?

     - Epoch in Machine Learning is used to indicate the count of passes in a given training dataset where the Machine Learning algorithm has done its job. Generally, when there is a large chunk of data, it is grouped into several batches. All these batches go through the given model, and this process is referred to as iteration. Now, if the batch size comprises the complete training dataset, then the count of iterations is the same as that of epochs.

166) How is the suitability of a Machine Learning Algorithm determined for a particular problem?

     - To identify a Machine Learning Algorithm for a particular problem, the following steps should be followed:

       * Step 1: Problem classification: Classification of the problem depends on the classification of input and output:
        Classifying the input: Classification of the input depends on whether there is data labeled (supervised learning) or unlabeled (unsupervised learning), or whether a model has to be created that interacts with the environment and improves itself (reinforcement learning.)
        Classifying the output: If the output of a model is required as a class, then some classification techniques need to be used.
        If the output is a number, then regression techniques must be used; if the output is a different cluster of inputs, then clustering techniques should be used.

       * Step 2: Checking the algorithms in hand: After classifying the problem, the available algorithms that can be deployed for solving the classified problem should be considered.

       * Step 3: Implementing the algorithms: If there are multiple algorithms available, then all of them are to be implemented. Finally, the algorithm that gives the best performance is selected.

167) What is the Variance Inflation Factor?

     - Variance inflation factor (VIF) is the estimate of the volume of multicollinearity in a collection of many regression variables.

        VIF = Variance of the model / Variance of the model with a single independent variable

        This ratio has to be calculated for every independent variable. If VIF is high, then it shows the high collinearity of the independent variables.

168) How to Standardize Data?

     - Standardization is the method that is used for rescaling data attributes. The attributes are likely to have a mean value of 0 and a value of the standard deviation of 1. The main objective of standardization is to prompt the mean and standard deviation for the attributes.

169) What's the "kernel trick" and how is it useful?

     - The Kernel trick involves kernel functions that can enable in higher-dimension spaces without explicitly calculating the coordinates of points within that dimension: instead, kernel functions compute the inner products between the images of all pairs of data in a feature space. This allows them the very useful attribute of calculating the coordinates of higher dimensions while being computationally cheaper than the explicit calculation of said coordinates. Many algorithms can be expressed in terms of inner products. Using the kernel trick enables us effectively run algorithms in a high-dimensional space with lower-dimensional data.

170) Describe a hash table.

     - A hash table is a data structure that produces an associative array. A key is mapped to certain values through the use of a hash function. They are often used for tasks such as database indexing.

171) What is the difference between Gini Impurity and Entropy in a Decision Tree?

     - Gini Impurity and Entropy are the metrics used for deciding how to split a Decision Tree.
    Gini measurement is the probability of a random sample being classified correctly if you randomly pick a label according to the distribution in the branch.
    Entropy is a measurement to calculate the lack of information. You calculate the Information Gain (difference in entropies) by making a split. This measure helps to reduce the uncertainty about the output label.

172) What is A/B Testing?

     - A/B is Statistical hypothesis testing for randomized experiment with two variables A and B. It is used to compare two models that use different predictor variables in order to check which variable fits best for a given sample of data.
Consider a scenario where you've created two models (using different predictor variables) that can be used to recommend products for an e-commerce platform.
A/B Testing can be used to compare these two models to check which one best recommends products to a customer.

173) How do we label the hidden layers of a neural network?

     - We label these intermediate or hidden layer nodes. The nodes are also called activation units.

174) What is feature extraction?

     - A method to transform or project the data onto a new feature space. In the context of dimensionality reduction, feature extraction can be understood as an approach to data compression with the goal of maintaining most of the relevant information.

175) What are Recurrent Neural Networks?

     - Recurrent Neural Networks (RNNs) can be thought of as feedforward neural networks with feedback loops or backpropagation through time. In RNNs, the neurons only fire for a limited amount of time before they are (temporarily) deactivated. In turn, these neurons activate other neurons that fire at a later point in time. Basically, we can think of recurrent neural networks as MLPs with an additional time variable. The time component and dynamic structure allows the network to use not only the current inputs but also the inputs that it encountered earlier.

176) What are the advantages of using a naive Bayes for classification?

        * Very simple, easy to implement and fast.
        * If the NB conditional independence assumption holds, then it will converge quicker than discriminative models like logistic regression.
        * Even if the NB assumption doesn’t hold, it works great in practice.
        Need less training data.
        * Highly scalable. It scales linearly with the number of predictors and data points.
        * Can be used for both binary and mult-iclass classification problems.
        * Can make probabilistic predictions.
        * Handles continuous and discrete data.
        * Not sensitive to irrelevant features.

177) In what real world applications is Naive Bayes classifier used?

     - Some of real world examples are as given below

        * To mark an email as spam, or not spam?
        * Classify a news article about technology, politics, or sports?
        * Check a piece of text expressing positive emotions, or negative emotions?
        * Also used for face recognition software

178) What do you understand by Precision and Recall?

     - In pattern recognition, The information retrieval and classification in machine learning are part of precision. It is also called as positive predictive value which is the fraction of relevant instances among the retrieved instances.

     - Recall is also known as sensitivity and the fraction of the total amount of relevant instances which  were actually retrieved. 

     - Both precision and recall are therefore based on an understanding and measure of relevance.

179) What Are the Three Stages of Building a Model in Machine Learning?

     - To build a model in machine learning, you need to follow few steps:

        * Understand the business model
        * Data acquisitions
        * Data cleaning
        * Exploratory data analysis
        * Use machine learning algorithms to make a model
        * Use unknown dataset to check the accuracy of the model

180) What is the difference between Entropy and Information Gain?

     - The information gain is based on the decrease in entropy after a dataset is split on an attribute. Constructing a decision tree is all about finding the attribute that returns the highest information gain (i.e., the most homogeneous branches). Step 1: Calculate entropy of the target.

181) What are collinearity and multicollinearity?

     - Collinearity is a linear association between two predictors. Multicollinearity is a situation where two or more predictors are highly linearly related.

182) What is Kernel SVM?

        - SVM algorithms have basically advantages in terms of complexity. First I would like to clear that both Logistic regression as well as SVM can form non linear decision surfaces and can be coupled with the kernel trick. If Logistic regression can be coupled with kernel then why use SVM?

        - SVM is found to have better performance practically in most cases.

        - SVM is computationally cheaper O(N^2*K) where K is no of support vectors (support vectors are those points that lie on the class margin) where as logistic regression is O(N^3)

        - Classifier in SVM depends only on a subset of points . Since we need to maximize distance between closest points of two classes (aka margin) we need to care about only a subset of points unlike logistic regression.

183) Why is “Naive” Bayes naive?

     - Despite its practical applications, especially in text mining, Naive Bayes is considered “Naive” because it makes an assumption that is virtually impossible to see in real-life data: the conditional probability is calculated as the pure product of the individual probabilities of components. This implies the absolute independence of features — a condition probably never met in real life.

     - As a Quora commenter put it whimsically, a Naive Bayes classifier that figured out that you liked pickles and ice cream would probably naively recommend you a pickle ice cream.


184) Explain how a ROC curve works.

     - The ROC curve is a graphical representation of the contrast between true positive rates and the false positive rate at various thresholds. It’s often used as a proxy for the trade-off between the sensitivity of the model (true positives) vs the fall-out or the probability it will trigger a false alarm (false positives).


185) What’s the difference between a generative and discriminative model?

     - A generative model will learn categories of data while a discriminative model will simply learn the distinction between different categories of data. Discriminative models will generally outperform generative models on classification tasks.


186) How is a decision tree pruned?

     - Pruning is what happens in decision trees when branches that have weak predictive power are removed in order to reduce the complexity of the model and increase the predictive accuracy of a decision tree model. Pruning can happen bottom-up and top-down, with approaches such as reduced error pruning and cost complexity pruning.

     - Reduced error pruning is perhaps the simplest version: replace each node. If it doesn’t decrease predictive accuracy, keep it pruned. While simple, this heuristic actually comes pretty close to an approach that would optimize for maximum accuracy.


187) What’s the F1 score? How would you use it?
     - The F1 score is a measure of a model’s performance. It is a weighted average of the precision and recall of a model, with results tending to 1 being the best, and those tending to 0 being the worst. You would use it in classification tests where true negatives don’t matter much.


188) How would you handle an imbalanced dataset?

     - An imbalanced dataset is when you have, for example, a classification test and 90% of the data is in one class. That leads to problems: an accuracy of 90% can be skewed if you have no predictive power on the other category of data! Here are a few tactics to get over the hump:

          * Collect more data to even the imbalances in the dataset.
          * Resample the dataset to correct for imbalances.
          * Try a different algorithm altogether on your dataset.


189) How do you ensure you’re not overfitting with a model?

     - This is a simple restatement of a fundamental problem in machine learning: the possibility of overfitting training data and carrying the noise of that data through to the test set, thereby providing inaccurate generalizations.

     - There are three main methods to avoid overfitting:

          * Keep the model simpler: reduce variance by taking into account fewer variables and parameters, thereby removing some of the noise in the training data.
          * Use cross-validation techniques such as k-folds cross-validation.
          * Use regularization techniques such as LASSO that penalize certain model parameters if they’re likely to cause overfitting.


190) What evaluation approaches would you work to gauge the effectiveness of a machine learning model?

     - You would first split the dataset into training and test sets, or perhaps use cross-validation techniques to further segment the dataset into composite sets of training and test sets within the data. You should then implement a choice selection of performance metrics: here is a fairly comprehensive list. You could use measures such as the F1 score, the accuracy, and the confusion matrix. What’s important here is to demonstrate that you understand the nuances of how a model is measured and how to choose the right performance measures for the right situations.


191) How do you handle missing or corrupted data in a dataset?

     - You could find missing/corrupted data in a dataset and either drop those rows or columns, or decide to replace them with another value.

     - In Pandas, there are two very useful methods: isnull() and dropna() that will help you find columns of data with missing or corrupted data and drop those values. If you want to fill the invalid values with a placeholder value (for example, 0), you could use the fillna() method.


192) Pick an algorithm. Write the pseudo-code for a parallel implementation.

     - This kind of question demonstrates your ability to think in parallelism and how you could handle concurrency in programming implementations dealing with big data. Take a look at pseudocode frameworks such as Peril-L and visualization tools such as Web Sequence Diagrams to help you demonstrate your ability to write code that reflects parallelism.


193) What are some differences between a linked list and an array?

     - An array is an ordered collection of objects. A linked list is a series of objects with pointers that direct how to process them sequentially. An array assumes that every element has the same size, unlike the linked list. A linked list can more easily grow organically: an array has to be pre-defined or re-defined for organic growth. Shuffling a linked list involves changing which points direct where—meanwhile, shuffling an array is more complex and takes more memory.


194) Which data visualization libraries do you use? What are your thoughts on the best data visualization tools?

     - What’s important here is to define your views on how to properly visualize data and your personal preferences when it comes to tools. Popular tools include R’s ggplot, Python’s seaborn and matplotlib, and tools such as Plot.ly and Tableau.


195) How are primary and foreign keys related in SQL?

     - Most machine learning engineers are going to have to be conversant with a lot of different data formats. SQL is still one of the key ones used. Your ability to understand how to manipulate SQL databases will be something you’ll most likely need to demonstrate. In this example, you can talk about how foreign keys allow you to match up and join tables together on the primary key of the corresponding table—but just as useful is to talk through how you would think about setting up SQL tables and querying them. 


196) How would you implement a recommendation system for our company’s users?

     - A lot of machine learning interview questions of this type will involve the implementation of machine learning models to a company’s problems. You’ll have to research the company and its industry in-depth, especially


197) What is a lambda expression in Python?

     - With the help of lambda expression, you can create an anonymous function. Unlike conventional functions, lambda functions occupy a single line of code. The basic syntax of a lambda function is –

     - lambda arguments: expression

       An example of lambda function in Python data science is –

               x = lambda a : a * 5
               print(x(5))

          We obtain the output of 25.


198) How will you measure the Euclidean distance between the two arrays in numpy?

     - In order to measure the Euclidean distance between the two arrays, we will first initialize our two arrays, then we will use the linalg.norm() function provided by the numpy library. Here, numpy is imported as np.

               a = np.array([1,2,3,4,5])
               b = np.array([6,7,8,9,10])
               # Solution
               e_dist = np.linalg.norm(a-b)
               e_dist
               11.180339887498949
     - With data integrity, we can define the accuracy as well as the consistency of the data. This integrity is to be ensured over the entire life-cycle.


199) How will you create an identity matrix using numpy?

     - In order to create the identity matrix with numpy, we will use the identity() function.
     - Numpy is imported as np

               np.identity(3)

          We will obtain the output as –

               array([[1., 0., 0.],
               [0., 1., 0.],
               [0., 0., 1.]])


200) You had mentioned Python as one of the tools for solving data science problems, can you tell me the various libraries of Python that are used in Data Science?

     - Some of the important libraries of Python that are used in Data Science are –

          * Numpy
          * SciPy
          * Pandas
          * Matplotlib
          * Keras 
          * TensorFlow
          * Scikit-learn

     
201) How do you create a 1-D array in numpy?

     - You can create a 1-D array in numpy as follows:

               x = np.array([1,2,3,4])

          Where numpy is imported as np


202) What function of numpy will you use to find maximum value from each row in a 2D numpy array?

     - In order to find the maximum value from each row in a 2D numpy array, we will use the amax() function as follows –

               np.amax(input, axis=1)

          Where numpy is imported as np and input is the input array.


203) Given two lists [1,2,3,4,5] and [6,7,8], you have to merge the list into a single dimension. How will you achieve this?

     - In order to merge the two lists into a single list, we will concatenate the two lists as follows –

               list1 + list2

          We will obtain the output as – [1, 2, 3, 4, 5, 6, 7, 8]


204) How will you create an identity matrix using numpy?

     - In order to create the identity matrix with numpy, we will use the identity() function. Numpy is imported as np

               np.identity(3)

          We will obtain the output as –

               array([[1., 0., 0.],
               [0., 1., 0.],
               [0., 0., 1.]])

     
205) How to add a border that is filled with 0s around an existing array?

      - In order to add a border to an array that is filled with 0s, we first make an array Z and initialize it with zeroes. We first import numpy as np.

               Z = np.ones((5,5))
               Then, we perform padding on it with the help of pad() function. 
               Z = np.pad(Z, pad_width=1, mode='constant', constant_values=0)
               print(Z)


206) How will you multiply a 4×3 matrix by a 3×2 matrix ?

     - There are two ways to do this. The first method is for the versions of Python that are older than 3.5 –

               Z = np.dot(np.ones((4,3)), np.ones((3,2)))
               print(Z)
               array([[3., 3.],
               [3., 3.],
               [3., 3.],
               [3., 3.]])

     - The second method is for Python version > 3.5,

               Z = np.ones((4,3)) @ np.ones((3,2))

          
207) Can you name the type of biases that occur in machine learning?

     - There are four main types of biases that occur while building machine learning algorithms –

          * Sample Bias
          * Prejudice Bias    
          * Measurement Bias
          * Algorithm Bias


208) How is skewness different from kurtosis?

     - In data science, the general meaning of skewness is basically to determine the imbalance. In statistics, skewness is a measure of asymmetry in the distribution of data. Ideally, data is normally distributed, meaning that both the left and right tails are equidistant from the center of the distribution. In this case, the skewness is 0. However, a distribution exhibits negative skewness if the left tail is longer than the right one. And, the distribution exhibits positive skewness if the right tail is longer than the left one.

     - In case of kurtosis, we measure the pointedness of the peak of distribution. The ideal kurtosis or the kurtosis of a normal distribution is 3. If the kurtosis of the tail data exceeds 3, then we say that the distributions possess heavy tails. And, if the kurtosis is less than 3, we say that the distributions have thin tails.


209) What is z-score?

     - Z-score, also known as the standard score is the number of standard deviations that the data-point is from the mean. It measures how many standard deviations below or above the population mean is. Z-score ranges from -3 and goes up till +3 standard deviations.






