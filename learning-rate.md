/ [Home](index.md)

# Learning Rate

Learning rate is a hyperparameter used to control the pace at which an algorithm updates the values of weights in a neural network concerning the loss function. A desirable learning rate is one that is low enough for a network to converge while being high enough to train in a reasonable time.

Various learning rate techniques:

- Decaying Learning Rate - the learning rate drops as the number of epochs increases
- Scheduled Drop Learning rate - the learning rate is lowered by a specified proportion at a specified frequency
- Cycling learning rate - the learning rate cyclically changes between a base rate and a maximum rate
- The Gradient Descent Method - the value of each parameter is originally assumed or assigned random values when training a model. The cost function is generated using the initial values, and the parameter estimations are improved over time so that the cost function eventually assumes a minimum value

<br>

**Created by Santhosh Kannan**

---

<br>
