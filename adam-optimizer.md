/ [Home](index.md)

# Adam Optimizer

Adaptive Moment Estimation(Adam) optimizer is an extension of Stochastic Gradient Descent algorithm that combines the optimizations made by Gradient Descent with Momentum and RMSprop optimizer. It is computationally efficient, requires little memory, and is well suited for problems that are large in terms of data.

<br>

**Created by Santhosh Kannan**

---

<br>
