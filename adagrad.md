/ [Home](index.md)

# AdaGrad

AdaGrad is an optimization technique based on Gradient Descent algorithm specially the Stochastic version(SGD). When some features of a model are more significant than others, SGD converges slowly as it treats all factors equally, regardless of the contribution of each factor to the output. To rectify this, AdaGrad was introduces which effectively gave each factor an independent learning rate. The learning rate of each factor is assigned relative to how frequent a parameter gets updated during learning.

<br>

**Created by Santhosh Kannan**

---

<br>
