/ [Home](index.md)

# Dropout Regularization

Deep learning neural networks have a high tendency to quickly overfit a training dataset. Ensemble techniques like having neural networks with different model configurations are known to reduce overfitting, but it is computationally expensive and maintaining the models is difficult.

Dropout regularization is the process of stimulating a large number of model configurations in a single model by dropping out nodes randomly during training. This method is computationally cheap and remarkably effective in reducing overfitting and improving generalization error in deep neural networks of all kinds.

<br>

**Created by Santhosh Kannan**

---

<br>
